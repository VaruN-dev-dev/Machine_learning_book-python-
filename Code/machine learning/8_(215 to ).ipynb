{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d9ff51",
   "metadata": {},
   "source": [
    "# Chapter 8: Dimensionality rediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96de45ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ig it is the problem when we have too many features\n",
    "\n",
    "# In short, the more dimensions the training set has, the greater the risk of overfitting\n",
    "# it.\n",
    "\n",
    "# bcoz in here more features means more complex things cause yk we plot a graph here i mean .. yk. but\n",
    "# in neural network we just map so there we don't have to worry about features that much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78584808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17d67e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following Python code uses NumPy’s svd() function to obtain all the principal\n",
    "# components of the training set, then extracts the first two PCs:\n",
    "\n",
    "# X_centered = X - X.mean(axis=0)\n",
    "# U, s, Vt = np.linalg.svd(X_centered)\n",
    "# c1 = Vt.T[:, 0]\n",
    "# c2 = Vt.T[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c3dc366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning :\n",
    "# PCA assumes that the dataset is centered around the origin. As we\n",
    "# will see, Scikit-Learn’s PCA classes take care of centering the data\n",
    "# for you. However, if you implement PCA yourself (as in the preceding\n",
    "# example), or if you use other libraries, don’t forget to center\n",
    "# the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f62c83b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 224\n",
    "# ..\n",
    "\n",
    "# Scikit-Learn’s PCA class implements PCA using SVD decomposition.\n",
    "# The following code applies PCA to reduce the dimensionality of the dataset\n",
    "# down to two dimensions (note that it automatically takes care of centering the data):\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components= 2)\n",
    "# X2D = pca.fit_transform(X)\n",
    "\n",
    "# After fitting the PCA transformer to the dataset, you can access the principal components\n",
    "# using the components_ variable (note that it contains the PCs as horizontal vec‐tors, so, for example, \n",
    "# the first principal component is equal to pca.components_.T[:,0])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9c01c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explained varience ration page->225\n",
    "\n",
    "# pca.explained_variance_ratio_\n",
    "# array([0.84248607, 0.14631839])\n",
    "# This tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6%\n",
    "# lies along the second axis. This leaves less than 1.2% for the third axis, so it is reasonable\n",
    "# to assume that it probably carries little information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dac10a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of arbitrarily choosing the number of dimensions to reduce down to, it is\n",
    "# generally preferable to choose the number of dimensions that add up to a sufficiently\n",
    "# large portion of the variance (e.g., 95%). Unless, of course, you are reducing dimensionality\n",
    "# for data visualization—in that case you will generally want to reduce the\n",
    "# dimensionality down to 2 or 3.\n",
    "\n",
    "# The following code computes PCA without reducing dimensionality, then computes\n",
    "# the minimum number of dimensions required to preserve 95% of the training set’s\n",
    "# variance:\n",
    "# pca = PCA()\n",
    "# pca.fit(X_train)\n",
    "# cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "# d = np.argmax(cumsum >= 0.95) + 1\n",
    "\n",
    "# You could then set n_components=d and run PCA again. However, there is a much\n",
    "# better option: instead of specifying the number of principal components you want to\n",
    "# preserve, you can set n_components to be a float between 0.0 and 1.0, indicating the\n",
    "# ratio of variance you wish to preserve:\n",
    "# pca = PCA(n_components=0.95)\n",
    "# X_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "# do see page->225, 226.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03a2aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca for compression page->226\n",
    "# do see from page->225,226 to 237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7024b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "mnist.target = mnist.target.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64c846aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dd66ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# The following code computes PCA without reducing dimensionality, then computes\n",
    "# the minimum number of dimensions required to preserve 95% of the training set’s\n",
    "# variance:\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "\n",
    "# You could then set n_components=d and run PCA again. However, there is a much\n",
    "# better option: instead of specifying the number of principal components you want to\n",
    "# preserve, you can set n_components to be a float between 0.0 and 1.0, indicating the\n",
    "# ratio of variance you wish to preserve:\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a5be73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obviously after dimensionality reduction, the training set takes up much less space.\n",
    "# For example, try applying PCA to the MNIST dataset while preserving 95% of its variance.\n",
    "# You should find that each instance will have just over 150 features, instead of\n",
    "# the original 784 features. So while most of the variance is preserved, the dataset is\n",
    "# now less than 20% of its original size! This is a reasonable compression ratio, and you\n",
    "# can see how this can speed up a classification algorithm (such as an SVM classifier)\n",
    "# tremendously.\n",
    "\n",
    "# the following code compresses the MNIST dataset down to 154 dimensions, then uses\n",
    "# the inverse_transform() method to decompress it back to 784 dimensions.\n",
    "# Figure 8-9 shows a few digits from the original training set (on the left), and the corresponding\n",
    "# digits after compression and decompression. You can see that there is a\n",
    "# slight image quality loss, but the digits are still mostly intact.\n",
    "\n",
    "pca = PCA(n_components = 154)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "X_recovered = pca.inverse_transform(X_reduced)\n",
    "\n",
    "# image on page->227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3aa11d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The problem with PCA was that we have to fit the whole data set but you can also fit the dataset in minibatches\n",
    "\n",
    "# The following code splits the MNIST dataset into 100 mini-batches (using NumPy’s\n",
    "# array_split() function) and feeds them to Scikit-Learn’s IncrementalPCA class5 to\n",
    "# reduce the dimensionality of the MNIST dataset down to 154 dimensions (just like\n",
    "# before). Note that you must call the partial_fit() method with each mini-batch\n",
    "# rather than the fit() method with the whole training set:\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "    \n",
    "X_reduced = inc_pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93b1ec47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52500, 784), (52500, 154))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_reduced.shape # see the features are reduced.\n",
    "# now we can fit these into our classifier which will train much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f2ad365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, you can use NumPy’s memmap class, which allows you to manipulate a\n",
    "# large array stored in a binary file on disk as if it were entirely in memory; the class\n",
    "# loads only the data it needs in memory, when it needs it. Since the IncrementalPCA\n",
    "# class uses only a small part of the array at any given time, the memory usage remains\n",
    "# under control. This makes it possible to call the usual fit() method, as you can see\n",
    "# in the following code:\n",
    "\n",
    "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\n",
    "batch_size = m // n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00976071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Chapter 5 we discussed the kernel trick, a mathematical technique that implicitly\n",
    "# maps instances into a very high-dimensional space (called the feature space), enabling\n",
    "# nonlinear classification and regression with Support Vector Machines. Recall that a\n",
    "# linear decision boundary in the high-dimensional feature space corresponds to a\n",
    "# complex nonlinear decision boundary in the original space.\n",
    "# It turns out that the same trick can be applied to PCA, making it possible to perform\n",
    "# complex nonlinear projections for dimensionality reduction. This is called Kernel\n",
    "# 6 “Kernel Principal Component Analysis,” B. Schölkopf, A. Smola, K. Müller (1999).\n",
    "# PCA (kPCA).6 It is often good at preserving clusters of instances after projection, or\n",
    "# sometimes even unrolling datasets that lie close to a twisted manifold.\n",
    "# For example, the following code uses Scikit-Learn’s KernelPCA class to perform kPCA\n",
    "# with an RBF kernel (see Chapter 5 for more details about the RBF kernel and the\n",
    "# other kernels):\n",
    "\n",
    "from sklearn.decomposition import KernelPCA\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "992c8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As kPCA is an unsupervised learning algorithm, there is no obvious performance\n",
    "# measure to help you select the best kernel and hyperparameter values. However,\n",
    "# dimensionality reduction is often a preparation step for a supervised learning task\n",
    "# (e.g., classification), so you can simply use grid search to select the kernel and hyperparameters\n",
    "# that lead to the best performance on that task. For example, the following\n",
    "# code creates a two-step pipeline, first reducing dimensionality to two dimensions\n",
    "# using kPCA, then applying Logistic Regression for classification. Then it uses Grid\n",
    "# SearchCV to find the best kernel and gamma value for kPCA in order to get the best\n",
    "# classification accuracy at the end of the pipeline:\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "(\"kpca\", KernelPCA(n_components=2)),\n",
    "(\"log_reg\", LogisticRegression())\n",
    "])\n",
    "param_grid = [{\n",
    "\"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    "\"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
    "}]\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bfea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may be wondering how to perform this reconstruction. One solution is to train a\n",
    "# supervised regression model, with the projected instances as the training set and the\n",
    "# original instances as the targets. Scikit-Learn will do this automatically if you set\n",
    "# fit_inverse_transform=True, as shown in the following code:\n",
    "\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433,\n",
    "fit_inverse_transform=True)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)\n",
    "By default, fit_inverse_transform=False and KernelPCA has no\n",
    "inverse_transform() method. This method only gets created\n",
    "when you set fit_inverse_transform=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2747577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(X, X_preimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ab1bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lle\n",
    "# some other techniques also..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
