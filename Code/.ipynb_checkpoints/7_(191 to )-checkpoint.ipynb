{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "378138ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db47136",
   "metadata": {},
   "source": [
    "# Chapter 7: Esemble learing and random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b259d2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group of predictor is called ensemble, thus, this technique is called \n",
    "# ensemble learning, and an ensemble learning algorith is called a ensemble method.\n",
    "\n",
    "# do read page->191\n",
    "\n",
    "# For example, you can train a group of Decision Tree classifiers, each on a different\n",
    "# random subset of the training set. To make predictions, you just obtain the predictions\n",
    "# of all individual trees, then predict the class that gets the most votes (see the last\n",
    "# exercise in Chapter 6). Such an ensemble of Decision Trees is called a Random Forest,\n",
    "# and despite its simplicity, this is one of the most powerful Machine Learning algorithms\n",
    "# available today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9087d879",
   "metadata": {},
   "source": [
    "## voting classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ecea7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read upto page->194\n",
    "# Ensemble methods work best when the predictors are as independent\n",
    "# from one another as possible. One way to get diverse classifiers\n",
    "# is to train them using very different algorithms. This increases the\n",
    "# chance that they will make very different types of errors, improving\n",
    "# the ensemble’s accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3661560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfb86ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following code creates and trains a voting classifier in Scikit-Learn, composed of\n",
    "# three diverse classifiers\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[(\"lr\", log_clf), (\"rf\", rnd_clf), (\"svc\", svm_clf)],\n",
    "voting=\"hard\")\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d3b610c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fe59767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.904\n",
      "VotingClassifier 0.896\n"
     ]
    }
   ],
   "source": [
    "# now lets look at each classifier's accuracy on the test set:\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e14cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do read page->194 really importand... \n",
    "# tells voting=\"hard\", voting=\"soft\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9742cdf",
   "metadata": {},
   "source": [
    "## Bagging and pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eab4b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One way to get a diverse set of classifiers is to use very different training algorithms,\n",
    "# as just discussed. Another approach is to use the same training algorithm for every\n",
    "# predictor, but to train them on different random subsets of the training set. When\n",
    "# sampling is performed with replacement, this method is called bagging1 (short for\n",
    "# bootstrap aggregating2). When sampling is performed without replacement, it is called\n",
    "# pasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3157278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n other words, both bagging and pasting allow training instances to be sampled several\n",
    "# times across multiple predictors, but only bagging allows training instances to be\n",
    "# sampled several times for the same predictor. This sampling and training process is\n",
    "# represented in Figure 7-4.\n",
    "# page->195"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb3dad9",
   "metadata": {},
   "source": [
    "## bagging and pasting in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1874201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-Learn offers a simple API for both bagging and pasting with the BaggingClas\n",
    "# sifier class (or BaggingRegressor for regression). The following code trains an\n",
    "# ensemble of 500 Decision Tree classifiers,5 each trained on 100 training instances randomly\n",
    "# sampled from the training set with replacement (this is an example of bagging,\n",
    "# but if you want to use pasting instead, just set bootstrap=False). The n_jobs parameter\n",
    "# tells Scikit-Learn the number of CPU cores to use for training and predictions\n",
    "# (–1 tells Scikit-Learn to use all available cores):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c5bd501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "DecisionTreeClassifier(), n_estimators=500,\n",
    "max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9be975",
   "metadata": {},
   "source": [
    "## out-of-bag evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5acf6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With bagging, some instances may be sampled several times for any given predictor,\n",
    "# while others may not be sampled at all. By default a BaggingClassifier samples m\n",
    "# training instances with replacement (bootstrap=True), where m is the size of the\n",
    "# training set. This means that only about 63% of the training instances are sampled on\n",
    "# average for each predictor.6 The remaining 37% of the training instances that are not\n",
    "# sampled are called out-of-bag (oob) instances. Note that they are not the same 37%\n",
    "# for all predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf27c671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=500,\n",
       "                  n_jobs=-1, oob_score=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since a predictor never sees the oob instances during training, it can be evaluated on\n",
    "# these instances, without the need for a separate validation set. You can evaluate the\n",
    "# ensemble itself by averaging out the oob evaluations of each predictor.\n",
    "# In Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier to\n",
    "# request an automatic oob evaluation after training. The following code demonstrates\n",
    "# this. The resulting evaluation score is available through the oob_score_ variable:\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "DecisionTreeClassifier(), n_estimators=500,\n",
    "bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ba56268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8986666666666666"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_score_\n",
    "# According to this oob evaluation, this BaggingClassifier is likely to achieve about\n",
    "# 89% accuracy on the test set. Let’s verify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "830cd869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b83d5567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31937173, 0.68062827],\n",
       "       [0.31460674, 0.68539326],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The oob decision function for each training instance is also available through the\n",
    "# oob_decision_function_ variable. In this case (since the base estimator has a pre\n",
    "# dict_proba() method) the decision function returns the class probabilities for each\n",
    "# training instance. For example, the oob evaluation estimates that the first training\n",
    "# instance has a 63.5% probability of belonging to the positive class (and 36.5% of\n",
    "# belonging to the negative class):\n",
    "\n",
    "bag_clf.oob_decision_function_[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84edc65d",
   "metadata": {},
   "source": [
    "## random patches and random subspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44d461bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just read the chapter from book cause there is a lot of theory involved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a5df68",
   "metadata": {},
   "source": [
    "## random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dfb55f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally\n",
    "# trained via the bagging method\n",
    "\n",
    "# The following code trains a\n",
    "# Random Forest classifier with 500 trees (each limited to maximum 16 nodes), using\n",
    "# all available CPU cores:\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,\n",
    "                                n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee736bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following BaggingClassifier is\n",
    "# roughly equivalent to the previous RandomForestClassifier:\n",
    "bag_clf = BaggingClassifier(\n",
    "DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
    "n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6fd0fd",
   "metadata": {},
   "source": [
    "## Extra trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6ef7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is hard to tell in advance whether a RandomForestClassifier\n",
    "# will perform better or worse than an ExtraTreesClassifier. Generally,\n",
    "# the only way to know is to try both and compare them using\n",
    "# cross-validation (and tuning the hyperparameters using grid\n",
    "# search).\n",
    "\n",
    "# page->200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aaf61f",
   "metadata": {},
   "source": [
    "## feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86a96dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.08995085653727478\n",
      "sepal width (cm) 0.02304238094998636\n",
      "petal length (cm) 0.42411520552570325\n",
      "petal width (cm) 0.4628915569870356\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "065208eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests are very handy to get a quick understanding of what features\n",
    "# actually matter, in particular if you need to perform feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca99597e",
   "metadata": {},
   "source": [
    "## boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "809688d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting (originally called hypothesis boosting) refers to any Ensemble method that\n",
    "# can combine several weak learners into a strong learner.\n",
    "# page->201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21b98814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most popular one are AdaBoost and Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022d37a2",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d4436f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generally skipped the math part behind it.. \n",
    "# from page->203 to 205..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cd73d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "# If your AdaBoost ensemble is overfitting the training set, you can\n",
    "# try reducing the number of estimators or more strongly regularizing\n",
    "# the base estimator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8ed32b",
   "metadata": {},
   "source": [
    "## Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0c15d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8670d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6441f6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff969707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae57bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a22e67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
