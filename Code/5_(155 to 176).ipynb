{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c574359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd19485",
   "metadata": {},
   "source": [
    "# Chapter 5: Support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a71202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('linear_svm', LinearSVC(C=1, loss='hinge'))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a smaller C values leads to a wider street but more margin\n",
    "# here C is for regularizaton\n",
    "\n",
    "# The following Scikit-Learn code loads the iris dataset, scales the features, and then\n",
    "# trains a linear SVM model (using the LinearSVC class with C = 1 and the hinge loss\n",
    "# function, described shortly) to detect Iris-Virginica flowers.\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # iris-virginica\n",
    "svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"linear_svm\", LinearSVC(C=1, loss=\"hinge\"))\n",
    "])\n",
    "svm_clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b95645ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5, 1.7]])\n",
    "# Unlike Logistic Regression classifiers, SVM classifiers do not output\n",
    "# probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b76b7d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, you could use the SVC class, using SVC(kernel=\"linear\", C=1), but it\n",
    "# is much slower, especially with large training sets, so it is not recommended. Another\n",
    "# option is to use the SGDClassifier class, with SGDClassifier(loss=\"hinge\",\n",
    "# alpha=1/(m*C)). This applies regular Stochastic Gradient Descent (see Chapter 4) to\n",
    "# train a linear SVM classifier. It does not converge as fast as the LinearSVC class, but it\n",
    "# can be useful to handle huge datasets that do not fit in memory (out-of-core training),\n",
    "# or to handle online classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93181dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------------------------\n",
    "# The LinearSVC class regularizes the bias term, so you should center\n",
    "# the training set first by subtracting its mean. This is automatic if\n",
    "# you scale the data using the StandardScaler. Moreover, make sure\n",
    "# you set the loss hyperparameter to \"hinge\", as it is not the default\n",
    "# value. Finally, for better performance you should set the dual\n",
    "# hyperparameter to False, unless there are more features than\n",
    "# training instances (we will discuss duality later in the chapter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caceed0",
   "metadata": {},
   "source": [
    "## Nonlinear SVM classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22e85f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although linear SVM classifiers are efficient and work surprisingly well in many\n",
    "# cases, many datasets are not even close to being linearly separable. One approach to\n",
    "# handling nonlinear datasets is to add more features, such as polynomial features\n",
    "# more on page->159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eee4e508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('poly_features', PolynomialFeatures(degree=3)),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('svm_clf', LinearSVC(C=10, loss='hinge'))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementing the above approach\n",
    "# Let’s test this on the moons\n",
    "# dataset: this is a toy dataset for binary classification in which the data points are shaped\n",
    "# as two interleaving half circles\n",
    "# You can generate this dataset\n",
    "# using the make_moons() function:\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "polynomial_svm_clf = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "     (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
    "])\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22f95229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page->160 a img.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390d90bc",
   "metadata": {},
   "source": [
    "## Polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82a8a222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=5, coef0=1, kernel='poly'))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fortunately, when using SVMs you can apply an almost miraculous mathematical\n",
    "# technique called the kernel trick (it is explained in a moment). It makes it possible to\n",
    "# get the same result as if you added many polynomial features, even with very highdegree\n",
    "# polynomials, without actually having to add them.\n",
    "# This trick is implemented by the SVC class.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "])\n",
    "poly_kernel_svm_clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "807e34e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page->161 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a616ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## gird serch page 79"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb95865",
   "metadata": {},
   "source": [
    "## adding similarity features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "854f4b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do check out page->161, 162\n",
    "# skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a57cab",
   "metadata": {},
   "source": [
    "## gaussian RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1549d133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=0.001, gamma=5))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "])\n",
    "rbf_kernel_svm_clf.fit(X, y)\n",
    "# Do check page->162, 163\n",
    "# So γ(gamma) acts like a regularization\n",
    "# hyperparameter: if your model is overfitting, you should reduce it, and if it is underfitting,\n",
    "# you should increase it (similar to the C hyperparameter).\n",
    "\n",
    "# Other kernels exist but are used much more rarely. For example, some kernels are\n",
    "# specialized for specific data structures. String kernels are sometimes used when classifying\n",
    "# text documents or DNA sequences (e.g., using the string subsequence kernel or\n",
    "# kernels based on the Levenshtein distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6c59420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With so many kernels to choose from, how can you decide which\n",
    "# one to use? As a rule of thumb, you should always try the linear\n",
    "# kernel first (remember that LinearSVC is much faster than SVC(ker\n",
    "# nel=\"linear\")), especially if the training set is very large or if it\n",
    "# has plenty of features. If the training set is not too large, you should\n",
    "# try the Gaussian RBF kernel as well; it works well in most cases.\n",
    "# Then if you have spare time and computing power, you can also\n",
    "# experiment with a few other kernels using cross-validation and grid\n",
    "# search, especially if there are kernels specialized for your training\n",
    "# set’s data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fe22e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page->164"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d39e088",
   "metadata": {},
   "source": [
    "## SVM regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01fc0f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a basic implementation\n",
    "# from sklearn.svm import LinearSVR\n",
    "# svm_reg = LinearSVR(epsilon=1.5)\n",
    "# svm_reg.fit(X, y)\n",
    "\n",
    "# *[LinearSVR is faster than SVR(kernel=\"linear\")] ig\n",
    "\n",
    "# ig for non-linear\n",
    "# from sklearn.svm import SVR\n",
    "# svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
    "# svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1ac646",
   "metadata": {},
   "source": [
    "## under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57cc7b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipping cause now no need .. \n",
    "# from page->166 to 176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc3a810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
