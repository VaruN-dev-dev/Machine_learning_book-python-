{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d9ff51",
   "metadata": {},
   "source": [
    "# Chapter 8: Dimensionality rediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96de45ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ig it is the problem when we have too many features\n",
    "\n",
    "# In short, the more dimensions the training set has, the greater the risk of overfitting\n",
    "# it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78584808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17d67e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following Python code uses NumPy’s svd() function to obtain all the principal\n",
    "# components of the training set, then extracts the first two PCs:\n",
    "\n",
    "# X_centered = X - X.mean(axis=0)\n",
    "# U, s, Vt = np.linalg.svd(X_centered)\n",
    "# c1 = Vt.T[:, 0]\n",
    "# c2 = Vt.T[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c3dc366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning :\n",
    "# PCA assumes that the dataset is centered around the origin. As we\n",
    "# will see, Scikit-Learn’s PCA classes take care of centering the data\n",
    "# for you. However, if you implement PCA yourself (as in the preceding\n",
    "# example), or if you use other libraries, don’t forget to center\n",
    "# the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f62c83b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 224\n",
    "# ..\n",
    "\n",
    "# Scikit-Learn’s PCA class implements PCA using SVD decomposition.\n",
    "# The following code applies PCA to reduce the dimensionality of the dataset\n",
    "# down to two dimensions (note that it automatically takes care of centering the data):\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components= 2)\n",
    "# X2D = pca.fit_transform(X)\n",
    "\n",
    "# After fitting the PCA transformer to the dataset, you can access the principal components\n",
    "# using the components_ variable (note that it contains the PCs as horizontal vec‐tors, so, for example, \n",
    "# the first principal component is equal to pca.components_.T[:,0])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9c01c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explained varience ration page->225\n",
    "\n",
    "# pca.explained_variance_ratio_\n",
    "# array([0.84248607, 0.14631839])\n",
    "# This tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6%\n",
    "# lies along the second axis. This leaves less than 1.2% for the third axis, so it is reasonable\n",
    "# to assume that it probably carries little information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dac10a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of arbitrarily choosing the number of dimensions to reduce down to, it is\n",
    "# generally preferable to choose the number of dimensions that add up to a sufficiently\n",
    "# large portion of the variance (e.g., 95%). Unless, of course, you are reducing dimensionality\n",
    "# for data visualization—in that case you will generally want to reduce the\n",
    "# dimensionality down to 2 or 3.\n",
    "\n",
    "# The following code computes PCA without reducing dimensionality, then computes\n",
    "# the minimum number of dimensions required to preserve 95% of the training set’s\n",
    "# variance:\n",
    "# pca = PCA()\n",
    "# pca.fit(X_train)\n",
    "# cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "# d = np.argmax(cumsum >= 0.95) + 1\n",
    "\n",
    "# You could then set n_components=d and run PCA again. However, there is a much\n",
    "# better option: instead of specifying the number of principal components you want to\n",
    "# preserve, you can set n_components to be a float between 0.0 and 1.0, indicating the\n",
    "# ratio of variance you wish to preserve:\n",
    "# pca = PCA(n_components=0.95)\n",
    "# X_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "# do see page->225, 226.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03a2aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca for compression page->226\n",
    "# do see from page->225,226 to 237"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
